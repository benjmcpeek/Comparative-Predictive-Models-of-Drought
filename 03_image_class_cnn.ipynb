{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06309ab3-0e89-49cd-95d3-a7342dd8b8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75dffb-a7d2-4d19-8715-c52d93553175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import cv2\n",
    "\n",
    "# images = [cv2.imread(file) for file in glob.glob('./data/train/nowildfire/*.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c85c1-c578-4fbf-bc7f-f0e4f789e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c927f-533e-4452-9e07-a06fbd9db928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a1c8b-7f95-4af3-9406-51f67087783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e454ab-5584-48c7-bb46-695a1cbd1285",
   "metadata": {},
   "source": [
    "# Part 3: Image Classification Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aef37c-8293-4e6b-9384-15d66482fd77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook Summary\n",
    "\n",
    "This notebook explores our second model in which satellite images are classified to identify whether an image has or has not experienced a wildfire. This second model assumes that wildfire areas exhibit many similar terrestrial characteristics to areas prone to drought, such a drier vegetation and soils as well as potentially sparser vegetation. If a convolutional nearual network model could be trained on wildfire and no wildfire satellite images, then a similar model coud be used on satellite images to predict drought. In this notebook, the reader will find:\n",
    "\n",
    "* Data Collection Methods\n",
    "* Image Preprocessing\n",
    "* Baseline CNN Model\n",
    "* Model Tuning\n",
    "* Production Model & Evaluation\n",
    "* Notebook Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41184f-e095-4cf0-9f8d-f6952c8121c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Collection Methods\n",
    "\n",
    "The following dataset was collected from a Kaggle Wildfire Prediction Dataset [source](https://www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset?select=train). The original dataset was compiled by the Kaggle contributor from Canada's government website, sourced from the government and municipalities of Quebec, which compiled images primarily from southern Quebec, dating back to 1976 [source](https://open.canada.ca/data/en/dataset/9d8f219c-4df0-4481-926f-8a2a532ca003). According to the Kaggle webpage, the wildfire images include those which contain greater than 0.01 acres burned. Upon cursory review of the images in both the \"wildfire\" and \"nowildfire\" classes, the \"nowildfire\" class contains images of both forested or green area as well as images of urban landscapes and human settlements, albeit not exclusively.\n",
    "\n",
    "Although the original Kaggle data contains three separate train, test, and validation datasets, we have downloaded only the train dataset since it contains 30,250 satellite images in total, of which 15,750 are classified as \"wildfire\" (\\~52%) and 14,500 are classified as \"nowildfire\" (\\~48%). The train dataset will be read into this notebook and then split into train and test datasets for training and evaluating our image classification CNN.\n",
    "\n",
    "In the subsequent section, we shall read in our image data and begin preprocessing any images, as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bd1f0-55c4-4691-93fe-8b838fee5a5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "In this section, we will begin reading in the dataset and then conducting any requisite cleaning of the data and preprocessing to make our models train and fit correctly. We will begin by importing the requisite libraries and using the image_dataset_from_directory model to create a train/test split and preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2e5dfb6-c538-48df-b9fc-b4f9f59a7220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requisite libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19843edf-4efe-4bd8-992d-3c4cecc19abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99f7dec9-1d6e-4874-9430-bd398f34c0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for file: .ipynb_checkpoints\n",
      "14499 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list\n",
    "nowildfire_arrays = []\n",
    "# define filepath for Dog class\n",
    "nowildfire_path = './data/train/nowildfire/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "for file in os.listdir(nowildfire_path):\n",
    "    try:\n",
    "        # target_size automatically resizes each img on import\n",
    "        nowildfire_img = load_img(nowildfire_path + file, target_size=(35, 35))\n",
    "        nowildfire_arr = img_to_array(nowildfire_img) / 255\n",
    "        nowildfire_arrays.append(nowildfire_arr)\n",
    "    except:\n",
    "        print(f'Error for file: {file}')\n",
    "\n",
    "print(f'{len(nowildfire_arrays)} pictures converted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28fc6a87-ff3e-4571-aa67-6ab1b2c61a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.627451  , 0.63529414, 0.5921569 ],\n",
       "        [0.67058825, 0.6627451 , 0.6156863 ],\n",
       "        [0.36078432, 0.3647059 , 0.29411766],\n",
       "        ...,\n",
       "        [0.7294118 , 0.72156864, 0.67058825],\n",
       "        [0.78039217, 0.77254903, 0.72156864],\n",
       "        [0.76862746, 0.7607843 , 0.7137255 ]],\n",
       "\n",
       "       [[0.6156863 , 0.62352943, 0.5803922 ],\n",
       "        [0.68235296, 0.6745098 , 0.627451  ],\n",
       "        [0.34117648, 0.34509805, 0.27450982],\n",
       "        ...,\n",
       "        [0.75686276, 0.7490196 , 0.69803923],\n",
       "        [0.7607843 , 0.7529412 , 0.7019608 ],\n",
       "        [0.75686276, 0.7490196 , 0.7019608 ]],\n",
       "\n",
       "       [[0.61960787, 0.62352943, 0.5921569 ],\n",
       "        [0.65882355, 0.6666667 , 0.6156863 ],\n",
       "        [0.30980393, 0.34117648, 0.25882354],\n",
       "        ...,\n",
       "        [0.7647059 , 0.75686276, 0.70980394],\n",
       "        [0.75686276, 0.7490196 , 0.7019608 ],\n",
       "        [0.7176471 , 0.7176471 , 0.6784314 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.4627451 , 0.4862745 , 0.47843137],\n",
       "        [0.65882355, 0.65882355, 0.6117647 ],\n",
       "        [0.58431375, 0.5882353 , 0.5176471 ],\n",
       "        ...,\n",
       "        [0.44705883, 0.44313726, 0.36078432],\n",
       "        [0.3529412 , 0.38431373, 0.29411766],\n",
       "        [0.38039216, 0.4       , 0.3137255 ]],\n",
       "\n",
       "       [[0.4862745 , 0.5294118 , 0.5137255 ],\n",
       "        [0.6627451 , 0.6784314 , 0.62352943],\n",
       "        [0.36862746, 0.3647059 , 0.29411766],\n",
       "        ...,\n",
       "        [0.3647059 , 0.37254903, 0.28627452],\n",
       "        [0.7372549 , 0.70980394, 0.6392157 ],\n",
       "        [0.8666667 , 0.8352941 , 0.7921569 ]],\n",
       "\n",
       "       [[0.4862745 , 0.50980395, 0.50980395],\n",
       "        [0.6627451 , 0.6784314 , 0.62352943],\n",
       "        [0.28235295, 0.31764707, 0.24313726],\n",
       "        ...,\n",
       "        [0.6901961 , 0.6666667 , 0.61960787],\n",
       "        [0.7058824 , 0.6901961 , 0.6431373 ],\n",
       "        [0.8784314 , 0.85490197, 0.80784315]]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowildfire_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edfc8437-f278-4893-a282-982a402bab23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 35, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowildfire_arrays[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b945cba9-dd63-4d23-b1f1-05ea9e4e0e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3675"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nowildfire_arrays[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a27c39b8-bbad-4e1e-aa69-5f9da8fcbff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape() takes exactly 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnowildfire_arrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape() takes exactly 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "nowildfire_arrays[0].reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91f7d74e-9ded-4b8c-b8a5-b40e241875aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 files belonging to 2 classes.\n",
      "Using 22688 files for training.\n",
      "Found 30250 files belonging to 2 classes.\n",
      "Using 7562 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# General Assembly instructor, Alanna Besaw, recommended investigating image_dataset_from_directory module for manipulating this dataset\n",
    "# original image sizes are 350 x 350 pixels\n",
    "# create a training set of the data representing 75% of the images\n",
    "img_train = image_dataset_from_directory('./data/train/',  \n",
    "                                         validation_split = 0.25,\n",
    "                                         image_size = (35, 35), # resized to exactly one tenth in size\n",
    "                                         subset = 'training', \n",
    "                                         seed = 42)\n",
    "\n",
    "# create a test set of the representing 25% of the images\n",
    "img_test = image_dataset_from_directory('./data/train/', \n",
    "                                         validation_split = 0.25, \n",
    "                                         image_size = (35, 35),  # resized to exactly one tenth in size\n",
    "                                         subset = 'validation', \n",
    "                                         seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5294e18f-0170-428c-91a4-cfa7d95e46f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 35, 35, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the datatype of the train set\n",
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1c94862-c2d5-49c0-979e-e95ba25c8453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 35, 35, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the datatype of the test set\n",
    "img_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fe533-6ce0-40d7-b1f0-664de81414fb",
   "metadata": {},
   "source": [
    "We now have the full image dataset in the form of a \\_BatchDataset data type and split into bothtrain and test sets. Upon trying to train our first CNN model, it kept throwing an error which was indicating a corrupted image file, so we searched Stack Overflow for a method to make a list of all the files in a directory. The code is attributed and executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8bfe567-6e54-4422-9ea2-299ea9593fa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this block of code was adapted from a Stack Overflow post at https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "directory = './data/train/nowildfire'\n",
    "\n",
    "# Get a list of all items (files and directories) in the directory\n",
    "all_items = os.listdir(directory)\n",
    "\n",
    "# Filter out only the file names from the list\n",
    "file_names = [item for item in all_items if os.path.isfile(os.path.join(directory, item))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99605314-7faa-449e-8e02-39000e2bdd37",
   "metadata": {},
   "source": [
    "After creating the list of files in the directory, we needed to iterate through all the files and identify which of the files might be corrupted and remove it from the directory. The following block of code is taken directly from Stack Overflow as well. The authors here have added comments to make sense of the code and how it identifies and removes a corrupted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1eb092cb-de7d-4db2-acfd-1fc3fc1625ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this block of was code was taken from a Stack Overflow post at https://stackoverflow.com/questions/62586443/tensorflow-error-when-trying-transfer-learning-invalid-jpeg-data-or-crop-windo\n",
    "\n",
    "# imports required to read in and unpack files\n",
    "from struct import unpack\n",
    "import os\n",
    "\n",
    "# maps parts of image onto markers\n",
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "# establishes the JPEG class\n",
    "class JPEG:\n",
    "    # initiates an object within the class by opening an image file and reading it in\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    # decodes an image by checking multiple markers to verify whether they are true\n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "            # print(marker_mapping.get(marker))\n",
    "            if marker == 0xffd8:\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9:\n",
    "                return\n",
    "            elif marker == 0xffda:\n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                data = data[2+lenchunk:]            \n",
    "            if len(data)==0:\n",
    "                break        \n",
    "\n",
    "# creates an empty list of bad images (corrupted data)\n",
    "bads = []\n",
    "\n",
    "# loops through the list of file names in the nowildfire directory\n",
    "# tries to decode the image to verify it is an uncorrupted jpg\n",
    "# if it fails to decode, adds corrupted jpg file to the bads list\n",
    "for img in file_names:\n",
    "  image = os.path.join(directory, img)\n",
    "  image = JPEG(image) \n",
    "  try:\n",
    "    image.decode()   \n",
    "  except:\n",
    "    bads.append(img)\n",
    "\n",
    "# loops through the bads list and removes corrupted files from directory\n",
    "for name in bads:\n",
    "  os.remove(os.path.join(directory,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed7d38ad-9e74-45cf-bc1a-012f06629ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the first execution of this code, only one file was added to the list but has since been removed\n",
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8434bc22-1da8-48ef-b96d-2c48c69a5fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nowildfire', 'wildfire']\n"
     ]
    }
   ],
   "source": [
    "class_names = img_train.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db060e8-2fa7-434b-98d9-be324a20b0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16204f5a-d550-4fd9-939b-cd40d98b4fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = Rescaling(1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a2ebae4-a887-48cc-9916-6660f14a3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.83921576\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = img_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_train_batch, labels_train_batch = next(iter(normalized_ds))\n",
    "first_image = image_train_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc18920d-04ba-468c-8ed6-f891b7e7381d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 35, 35, 3), dtype=float32, numpy=\n",
       "array([[[[0.1637255 , 0.21666668, 0.12843138],\n",
       "         [0.00490196, 0.1392157 , 0.01176471],\n",
       "         [0.21862747, 0.34509805, 0.18235295],\n",
       "         ...,\n",
       "         [0.47156864, 0.5852941 , 0.4539216 ],\n",
       "         [0.5784314 , 0.5735294 , 0.454902  ],\n",
       "         [0.47450984, 0.4156863 , 0.3421569 ]],\n",
       "\n",
       "        [[0.18529412, 0.3147059 , 0.19705884],\n",
       "         [0.3421569 , 0.39705884, 0.32450983],\n",
       "         [0.31274512, 0.36666667, 0.33235297],\n",
       "         ...,\n",
       "         [0.37549022, 0.47450984, 0.34607846],\n",
       "         [0.08431373, 0.2529412 , 0.07843138],\n",
       "         [0.15784314, 0.34803924, 0.15882353]],\n",
       "\n",
       "        [[0.41176474, 0.46274513, 0.3647059 ],\n",
       "         [0.2911765 , 0.39607847, 0.2901961 ],\n",
       "         [0.5294118 , 0.56274515, 0.49509805],\n",
       "         ...,\n",
       "         [0.1382353 , 0.3284314 , 0.14803922],\n",
       "         [0.12843138, 0.30882356, 0.12745099],\n",
       "         [0.1382353 , 0.32156864, 0.13431373]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.48823532, 0.5254902 , 0.44215688],\n",
       "         [0.62254906, 0.64607847, 0.54901963],\n",
       "         [0.6421569 , 0.64607847, 0.5833334 ],\n",
       "         ...,\n",
       "         [0.5421569 , 0.57549024, 0.48431376],\n",
       "         [0.427451  , 0.45784315, 0.39803925],\n",
       "         [0.2784314 , 0.39313728, 0.27058825]],\n",
       "\n",
       "        [[0.53039217, 0.54607844, 0.46960786],\n",
       "         [0.6058824 , 0.61078435, 0.5196079 ],\n",
       "         [0.46078435, 0.41274512, 0.3401961 ],\n",
       "         ...,\n",
       "         [0.22352943, 0.27156866, 0.22843139],\n",
       "         [0.3901961 , 0.37254903, 0.31862748],\n",
       "         [0.18235295, 0.2656863 , 0.14411765]],\n",
       "\n",
       "        [[0.6       , 0.59803927, 0.54509807],\n",
       "         [0.64019614, 0.6558824 , 0.59705883],\n",
       "         [0.5558824 , 0.57156867, 0.504902  ],\n",
       "         ...,\n",
       "         [0.5686275 , 0.63431376, 0.5647059 ],\n",
       "         [0.01666667, 0.1137255 , 0.02058824],\n",
       "         [0.10588236, 0.28137258, 0.11470589]]],\n",
       "\n",
       "\n",
       "       [[[0.33333334, 0.36078432, 0.30588236],\n",
       "         [0.01666667, 0.09313726, 0.04019608],\n",
       "         [0.15882353, 0.25882354, 0.19411767],\n",
       "         ...,\n",
       "         [0.58725494, 0.5176471 , 0.4411765 ],\n",
       "         [0.21176472, 0.2764706 , 0.18431373],\n",
       "         [0.07058824, 0.20686276, 0.09215686]],\n",
       "\n",
       "        [[0.13235295, 0.1254902 , 0.08137255],\n",
       "         [0.11862746, 0.17745098, 0.1137255 ],\n",
       "         [0.2892157 , 0.4647059 , 0.25980395],\n",
       "         ...,\n",
       "         [0.06078432, 0.18627451, 0.0882353 ],\n",
       "         [0.19215688, 0.31764707, 0.19509806],\n",
       "         [0.0254902 , 0.1009804 , 0.02843137]],\n",
       "\n",
       "        [[0.18529412, 0.3372549 , 0.1872549 ],\n",
       "         [0.18431373, 0.29803923, 0.20098041],\n",
       "         [0.10686275, 0.24215688, 0.18529412],\n",
       "         ...,\n",
       "         [0.10490197, 0.28235295, 0.13137256],\n",
       "         [0.18823531, 0.30784315, 0.19803923],\n",
       "         [0.60882354, 0.6950981 , 0.6       ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.18137255, 0.33627453, 0.14215687],\n",
       "         [0.3509804 , 0.4147059 , 0.26078433],\n",
       "         [0.1637255 , 0.33431375, 0.14313726],\n",
       "         ...,\n",
       "         [0.14117648, 0.31078434, 0.17156863],\n",
       "         [0.5264706 , 0.55098045, 0.47156864],\n",
       "         [0.33235297, 0.3892157 , 0.31078434]],\n",
       "\n",
       "        [[0.01372549, 0.13529412, 0.01176471],\n",
       "         [0.21764708, 0.33137256, 0.18627451],\n",
       "         [0.27058825, 0.3509804 , 0.19607845],\n",
       "         ...,\n",
       "         [0.5539216 , 0.57450986, 0.5137255 ],\n",
       "         [0.377451  , 0.4284314 , 0.3921569 ],\n",
       "         [0.17156863, 0.22352943, 0.18627451]],\n",
       "\n",
       "        [[0.10588236, 0.24411766, 0.11176471],\n",
       "         [0.11274511, 0.24803923, 0.10980393],\n",
       "         [0.3509804 , 0.3745098 , 0.2019608 ],\n",
       "         ...,\n",
       "         [0.2656863 , 0.31274512, 0.27352944],\n",
       "         [0.5833334 , 0.6264706 , 0.5656863 ],\n",
       "         [0.28431374, 0.3509804 , 0.25      ]]],\n",
       "\n",
       "\n",
       "       [[[0.14803922, 0.2029412 , 0.11274511],\n",
       "         [0.02156863, 0.04509804, 0.03333334],\n",
       "         [0.01470588, 0.04117647, 0.0382353 ],\n",
       "         ...,\n",
       "         [0.01372549, 0.02941177, 0.0254902 ],\n",
       "         [0.01960784, 0.03529412, 0.03137255],\n",
       "         [0.02352941, 0.03921569, 0.03529412]],\n",
       "\n",
       "        [[0.10882354, 0.15784314, 0.08921569],\n",
       "         [0.01568628, 0.04019608, 0.02156863],\n",
       "         [0.04117647, 0.08627451, 0.0382353 ],\n",
       "         ...,\n",
       "         [0.01568628, 0.03333334, 0.02352941],\n",
       "         [0.01960784, 0.03529412, 0.03137255],\n",
       "         [0.01960784, 0.03529412, 0.03137255]],\n",
       "\n",
       "        [[0.01862745, 0.04607843, 0.01764706],\n",
       "         [0.01764706, 0.04313726, 0.02156863],\n",
       "         [0.16568628, 0.22254904, 0.14803922],\n",
       "         ...,\n",
       "         [0.02647059, 0.04019608, 0.03431373],\n",
       "         [0.01568628, 0.04313726, 0.01568628],\n",
       "         [0.02647059, 0.05490196, 0.02058824]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.01568628, 0.03137255, 0.02745098],\n",
       "         [0.01960784, 0.03529412, 0.03137255],\n",
       "         [0.02058824, 0.04313726, 0.03137255],\n",
       "         ...,\n",
       "         [0.2539216 , 0.33235297, 0.22254904],\n",
       "         [0.15784314, 0.25      , 0.13039216],\n",
       "         [0.17843138, 0.24705884, 0.12156864]],\n",
       "\n",
       "        [[0.01568628, 0.03137255, 0.02745098],\n",
       "         [0.01568628, 0.03137255, 0.02745098],\n",
       "         [0.01764706, 0.0372549 , 0.02156863],\n",
       "         ...,\n",
       "         [0.15980393, 0.24509805, 0.11470589],\n",
       "         [0.16764706, 0.26176473, 0.13431373],\n",
       "         [0.28529415, 0.35980394, 0.25      ]],\n",
       "\n",
       "        [[0.01960784, 0.03529412, 0.03137255],\n",
       "         [0.01764706, 0.03333334, 0.02941177],\n",
       "         [0.01372549, 0.02941177, 0.0254902 ],\n",
       "         ...,\n",
       "         [0.09901962, 0.14117648, 0.04215686],\n",
       "         [0.27549022, 0.35000002, 0.22450982],\n",
       "         [0.17843138, 0.28431374, 0.14705883]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.19509806, 0.30588236, 0.20686276],\n",
       "         [0.18137255, 0.2656863 , 0.21568629],\n",
       "         [0.17058824, 0.26666668, 0.15882353],\n",
       "         ...,\n",
       "         [0.14901961, 0.2784314 , 0.16470589],\n",
       "         [0.1382353 , 0.27549022, 0.14607844],\n",
       "         [0.11862746, 0.26372552, 0.14215687]],\n",
       "\n",
       "        [[0.17450981, 0.29901963, 0.17254902],\n",
       "         [0.17450981, 0.2392157 , 0.20000002],\n",
       "         [0.19901963, 0.2647059 , 0.18039216],\n",
       "         ...,\n",
       "         [0.11862746, 0.25588238, 0.1382353 ],\n",
       "         [0.13725491, 0.25784317, 0.13039216],\n",
       "         [0.14215687, 0.26372552, 0.14215687]],\n",
       "\n",
       "        [[0.16862746, 0.29803923, 0.15294118],\n",
       "         [0.17450981, 0.27156866, 0.20000002],\n",
       "         [0.19411767, 0.2519608 , 0.17941177],\n",
       "         ...,\n",
       "         [0.18529412, 0.2911765 , 0.16176471],\n",
       "         [0.10784315, 0.2509804 , 0.12254903],\n",
       "         [0.1264706 , 0.25588238, 0.13431373]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3892157 , 0.4009804 , 0.28725493],\n",
       "         [0.44803923, 0.45000002, 0.32941177],\n",
       "         [0.5372549 , 0.52156866, 0.38431376],\n",
       "         ...,\n",
       "         [0.32352942, 0.4166667 , 0.29411766],\n",
       "         [0.19117649, 0.31274512, 0.19117649],\n",
       "         [0.17058824, 0.28627452, 0.18529412]],\n",
       "\n",
       "        [[0.2892157 , 0.33137256, 0.19803923],\n",
       "         [0.4431373 , 0.4382353 , 0.30882356],\n",
       "         [0.5254902 , 0.5058824 , 0.3803922 ],\n",
       "         ...,\n",
       "         [0.35490197, 0.4294118 , 0.3529412 ],\n",
       "         [0.12058824, 0.23039217, 0.13235295],\n",
       "         [0.24313727, 0.3372549 , 0.19803923]],\n",
       "\n",
       "        [[0.454902  , 0.43137258, 0.3156863 ],\n",
       "         [0.28431374, 0.34313726, 0.20000002],\n",
       "         [0.5       , 0.48431376, 0.3656863 ],\n",
       "         ...,\n",
       "         [0.23725492, 0.3284314 , 0.20980394],\n",
       "         [0.22941178, 0.3392157 , 0.18627451],\n",
       "         [0.20784315, 0.30784315, 0.20490198]]],\n",
       "\n",
       "\n",
       "       [[[0.33823532, 0.34509805, 0.2656863 ],\n",
       "         [0.34607846, 0.35588238, 0.27549022],\n",
       "         [0.48235297, 0.4911765 , 0.4284314 ],\n",
       "         ...,\n",
       "         [0.45686278, 0.48725492, 0.40784317],\n",
       "         [0.28235295, 0.32058826, 0.22941178],\n",
       "         [0.47941178, 0.51274514, 0.40784317]],\n",
       "\n",
       "        [[0.3539216 , 0.37647063, 0.29705885],\n",
       "         [0.5019608 , 0.5137255 , 0.4656863 ],\n",
       "         [0.3901961 , 0.40980396, 0.35980394],\n",
       "         ...,\n",
       "         [0.7372549 , 0.74509805, 0.6803922 ],\n",
       "         [0.13529412, 0.16862746, 0.09215686],\n",
       "         [0.13039216, 0.15588236, 0.13431373]],\n",
       "\n",
       "        [[0.26176473, 0.28333336, 0.20490198],\n",
       "         [0.23627453, 0.27450982, 0.17450981],\n",
       "         [0.31960785, 0.3372549 , 0.1872549 ],\n",
       "         ...,\n",
       "         [0.77647066, 0.76470596, 0.6901961 ],\n",
       "         [0.10784315, 0.15294118, 0.0627451 ],\n",
       "         [0.7039216 , 0.6686275 , 0.6039216 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.1137255 , 0.15196079, 0.09901962],\n",
       "         [0.07843138, 0.18039216, 0.07450981],\n",
       "         [0.477451  , 0.5107843 , 0.4058824 ],\n",
       "         ...,\n",
       "         [0.84803927, 0.84019613, 0.77156866],\n",
       "         [0.58137256, 0.5735294 , 0.49705884],\n",
       "         [0.47450984, 0.46274513, 0.4039216 ]],\n",
       "\n",
       "        [[0.10294119, 0.15882353, 0.08235294],\n",
       "         [0.25686276, 0.37647063, 0.19411767],\n",
       "         [0.5058824 , 0.5039216 , 0.4431373 ],\n",
       "         ...,\n",
       "         [0.44215688, 0.46078435, 0.36666667],\n",
       "         [0.13137256, 0.17450981, 0.09117647],\n",
       "         [0.29215688, 0.30392158, 0.22941178]],\n",
       "\n",
       "        [[0.18235295, 0.28039217, 0.16470589],\n",
       "         [0.3019608 , 0.36666667, 0.25686276],\n",
       "         [0.532353  , 0.51862746, 0.47058827],\n",
       "         ...,\n",
       "         [0.2882353 , 0.3029412 , 0.2137255 ],\n",
       "         [0.27254903, 0.29607844, 0.21666668],\n",
       "         [0.21764708, 0.29607844, 0.1764706 ]]],\n",
       "\n",
       "\n",
       "       [[[0.04117647, 0.14117648, 0.09509805],\n",
       "         [0.1009804 , 0.16470589, 0.17843138],\n",
       "         [0.4058824 , 0.5421569 , 0.49607846],\n",
       "         ...,\n",
       "         [0.20686276, 0.3421569 , 0.24901962],\n",
       "         [0.05784314, 0.13725491, 0.13137256],\n",
       "         [0.21862747, 0.30980393, 0.14117648]],\n",
       "\n",
       "        [[0.0127451 , 0.11960785, 0.03137255],\n",
       "         [0.21862747, 0.3254902 , 0.24803923],\n",
       "         [0.04215686, 0.10000001, 0.11862746],\n",
       "         ...,\n",
       "         [0.28725493, 0.43235296, 0.33235297],\n",
       "         [0.07450981, 0.24705884, 0.09117647],\n",
       "         [0.35784316, 0.42156866, 0.277451  ]],\n",
       "\n",
       "        [[0.08921569, 0.2019608 , 0.10588236],\n",
       "         [0.21862747, 0.3254902 , 0.16176471],\n",
       "         [0.45098042, 0.46862748, 0.2509804 ],\n",
       "         ...,\n",
       "         [0.36078432, 0.50686276, 0.46764708],\n",
       "         [0.34803924, 0.454902  , 0.39803925],\n",
       "         [0.11568628, 0.2647059 , 0.10588236]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.04215686, 0.0382353 ],\n",
       "         [0.09117647, 0.19705884, 0.14411765],\n",
       "         [0.01372549, 0.13529412, 0.07058824],\n",
       "         ...,\n",
       "         [0.00196078, 0.0382353 , 0.02941177],\n",
       "         [0.03333334, 0.12745099, 0.10196079],\n",
       "         [0.0254902 , 0.10196079, 0.05980393]],\n",
       "\n",
       "        [[0.08333334, 0.21960786, 0.1627451 ],\n",
       "         [0.00098039, 0.0382353 , 0.05686275],\n",
       "         [0.11176471, 0.25490198, 0.1509804 ],\n",
       "         ...,\n",
       "         [0.01078431, 0.04901961, 0.06078432],\n",
       "         [0.01176471, 0.02352941, 0.04411765],\n",
       "         [0.0127451 , 0.08921569, 0.06764706]],\n",
       "\n",
       "        [[0.37156865, 0.49313727, 0.43235296],\n",
       "         [0.06176471, 0.25      , 0.12450981],\n",
       "         [0.08137255, 0.17058824, 0.13627452],\n",
       "         ...,\n",
       "         [0.11960785, 0.23039217, 0.11568628],\n",
       "         [0.00686275, 0.02058824, 0.06372549],\n",
       "         [0.02156863, 0.01764706, 0.08039216]]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0807c543-7eed-4429-a6b7-c7d27eca41da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       "array([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1], dtype=int32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f146233-1762-43b4-8909-17bd20d72700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021568628 0.9088236\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = img_test.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_test_batch, labels_test_batch = next(iter(normalized_ds))\n",
    "first_image = image_test_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70410af7-6dd8-42b4-8b01-133ecec31b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Make a convolutional layer with 32 filters.\n",
    "model.add(Conv2D(32, 3, activation='relu', input_shape=(35, 35, 3)))\n",
    "\n",
    "# MaxPool the results (basically a requirement)\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Let's add another convolution block\n",
    "model.add(Conv2D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Finally, flatten the output and make a predictions through a dense layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f87851bd-28cf-4693-9615-cd2e018af8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='rmsprop',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8cdc6896-1ccf-4dac-b442-bebc0507d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_train_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_test_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1047\u001b[0m, in \u001b[0;36m_EagerTensorBase.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1047\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit!\n",
    "history = model.fit(\n",
    "    image_train_batch,\n",
    "    validation_data=(image_test_batch),\n",
    "    batch_size=512,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabf0cb-569c-459b-bbce-87a709bcc4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c00b955-6452-484f-94a1-b30900908966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac04bc72-d2a3-4751-812c-4fd4903e1145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1221c5d6-edf5-440d-9f65-91380b9434b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "709/709 [==============================] - 11s 14ms/step - loss: 0.8152 - acc: 0.8180 - val_loss: 0.4151 - val_acc: 0.8412\n",
      "Epoch 2/10\n",
      "709/709 [==============================] - 19s 26ms/step - loss: 0.2909 - acc: 0.8862 - val_loss: 0.2602 - val_acc: 0.9033\n",
      "Epoch 3/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2758 - acc: 0.8935 - val_loss: 0.2482 - val_acc: 0.9084\n",
      "Epoch 4/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2740 - acc: 0.8988 - val_loss: 0.2695 - val_acc: 0.8947\n",
      "Epoch 5/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2610 - acc: 0.9013 - val_loss: 0.3031 - val_acc: 0.8947\n",
      "Epoch 6/10\n",
      "709/709 [==============================] - 15s 21ms/step - loss: 0.2498 - acc: 0.9074 - val_loss: 0.2428 - val_acc: 0.9143\n",
      "Epoch 7/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2464 - acc: 0.9079 - val_loss: 0.2541 - val_acc: 0.9094\n",
      "Epoch 8/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2601 - acc: 0.9096 - val_loss: 0.2455 - val_acc: 0.9078\n",
      "Epoch 9/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2586 - acc: 0.9071 - val_loss: 0.2717 - val_acc: 0.9037\n",
      "Epoch 10/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2558 - acc: 0.9102 - val_loss: 0.2387 - val_acc: 0.9171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Fit!\n",
    "# history = model.fit(\n",
    "#     img_train,\n",
    "#     validation_data=(img_test),\n",
    "#     batch_size=128,\n",
    "#     epochs=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ba2f6-4096-4366-aea7-71d6a10acf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1cb8a-bd99-4036-baee-53172ea75aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ef6ae-acef-4d40-aac2-71172eb18d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a6bd4-3eb2-4361-8c52-c67e0e849ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaac16-f2f3-4e31-a0d1-859e5f434088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd5525-3679-43d5-b770-4f3afa1f6604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ae802-2c28-458e-8897-f4bbdc563f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27703e8-0b77-4687-80be-a8bf5e2ad00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e8460-f128-4a03-8253-17375c5601c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee61c35-10e6-4e49-bab5-cda301f3745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef842cec-5be2-4c29-be4c-0006091223a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea109-2cc1-46d8-a8ee-42302d7b66cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda3aac-d9ab-4c18-ab4f-a9a265aabf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaab15-6fba-454f-9a85-ca9d7459d239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f953dca-864b-4b1c-a666-15ca86bebabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638633f9-eb26-49f0-aea0-450014c323e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442193f9-3f17-4668-8583-c6261c602285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056d0b1-1182-48a2-8f23-310f6e4b8a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002616ef-b97a-463f-9415-997bd18a6108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bf29d-7841-4078-8bf9-9c16dfa934b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a3d74-8aa9-413e-9321-02d354ac479e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
