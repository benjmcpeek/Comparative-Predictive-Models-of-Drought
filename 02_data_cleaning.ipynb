{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06309ab3-0e89-49cd-95d3-a7342dd8b8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e5dfb6-c538-48df-b9fc-b4f9f59a7220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f7d74e-9ded-4b8c-b8a5-b40e241875aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n"
     ]
    }
   ],
   "source": [
    "# import glob\n",
    "# import cv2\n",
    "\n",
    "# images = [cv2.imread(file) for file in glob.glob('./data/train/nowildfire/*.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5294e18f-0170-428c-91a4-cfa7d95e46f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8bfe567-6e54-4422-9ea2-299ea9593fa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[182, 190, 189],\n",
       "        [207, 215, 214],\n",
       "        [161, 169, 168],\n",
       "        ...,\n",
       "        [181, 193, 195],\n",
       "        [180, 192, 194],\n",
       "        [179, 191, 193]],\n",
       "\n",
       "       [[182, 190, 189],\n",
       "        [206, 214, 213],\n",
       "        [161, 169, 168],\n",
       "        ...,\n",
       "        [181, 193, 195],\n",
       "        [180, 192, 194],\n",
       "        [179, 191, 193]],\n",
       "\n",
       "       [[182, 190, 189],\n",
       "        [205, 213, 212],\n",
       "        [160, 168, 167],\n",
       "        ...,\n",
       "        [181, 193, 195],\n",
       "        [179, 191, 193],\n",
       "        [179, 191, 193]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 70,  67,  59],\n",
       "        [ 71,  68,  60],\n",
       "        [ 69,  66,  58],\n",
       "        ...,\n",
       "        [207, 219, 225],\n",
       "        [204, 216, 222],\n",
       "        [199, 211, 217]],\n",
       "\n",
       "       [[ 69,  66,  58],\n",
       "        [ 72,  69,  61],\n",
       "        [ 68,  65,  57],\n",
       "        ...,\n",
       "        [207, 219, 225],\n",
       "        [204, 216, 222],\n",
       "        [199, 211, 217]],\n",
       "\n",
       "       [[ 68,  65,  57],\n",
       "        [ 73,  70,  62],\n",
       "        [ 67,  64,  56],\n",
       "        ...,\n",
       "        [208, 220, 226],\n",
       "        [205, 217, 223],\n",
       "        [200, 212, 218]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb092cb-de7d-4db2-acfd-1fc3fc1625ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed7d38ad-9e74-45cf-bc1a-012f06629ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 files belonging to 2 classes.\n",
      "Using 22688 files for training.\n",
      "Found 30250 files belonging to 2 classes.\n",
      "Using 7562 files for validation.\n"
     ]
    }
   ],
   "source": [
    "img_train = image_dataset_from_directory('./data/train/',  \n",
    "                                         validation_split = 0.25,\n",
    "                                         image_size = (28, 28),\n",
    "                                         subset = 'training', \n",
    "                                         seed = 42)\n",
    "img_test = image_dataset_from_directory('./data/train/', \n",
    "                                         validation_split = 0.25, \n",
    "                                         image_size = (28, 28),\n",
    "                                         subset = 'validation', \n",
    "                                         seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8434bc22-1da8-48ef-b96d-2c48c69a5fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16204f5a-d550-4fd9-939b-cd40d98b4fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = './data/train/nowildfire'\n",
    "\n",
    "# Get a list of all items (files and directories) in the directory\n",
    "all_items = os.listdir(directory)\n",
    "\n",
    "# Filter out only the file names from the list\n",
    "file_names = [item for item in all_items if os.path.isfile(os.path.join(directory, item))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a2ebae4-a887-48cc-9916-6660f14a3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from struct import unpack\n",
    "# from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "\n",
    "class JPEG:\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "            # print(marker_mapping.get(marker))\n",
    "            if marker == 0xffd8:\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9:\n",
    "                return\n",
    "            elif marker == 0xffda:\n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                data = data[2+lenchunk:]            \n",
    "            if len(data)==0:\n",
    "                break        \n",
    "\n",
    "\n",
    "bads = []\n",
    "\n",
    "\n",
    "for img in file_names:\n",
    "  image = os.path.join(directory, img)\n",
    "  image = JPEG(image) \n",
    "  try:\n",
    "    image.decode()   \n",
    "  except:\n",
    "    bads.append(img)\n",
    "\n",
    "\n",
    "for name in bads:\n",
    "  os.remove(os.path.join(directory,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0807c543-7eed-4429-a6b7-c7d27eca41da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-114.152378,51.027198.jpg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70410af7-6dd8-42b4-8b01-133ecec31b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f87851bd-28cf-4693-9615-cd2e018af8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nowildfire', 'wildfire']\n"
     ]
    }
   ],
   "source": [
    "class_names = img_train.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cdc6896-1ccf-4dac-b442-bebc0507d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adabf0cb-569c-459b-bbce-87a709bcc4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.84583336\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = img_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c00b955-6452-484f-94a1-b30900908966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Make a convolutional layer with 32 filters.\n",
    "model.add(Conv2D(32, 3, activation='relu', input_shape=(28, 28, 3)))\n",
    "\n",
    "# MaxPool the results (basically a requirement)\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Let's add another convolution block\n",
    "model.add(Conv2D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Finally, flatten the output and make a predictions through a dense layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac04bc72-d2a3-4751-812c-4fd4903e1145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='rmsprop',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1221c5d6-edf5-440d-9f65-91380b9434b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "709/709 [==============================] - 11s 14ms/step - loss: 0.8152 - acc: 0.8180 - val_loss: 0.4151 - val_acc: 0.8412\n",
      "Epoch 2/10\n",
      "709/709 [==============================] - 19s 26ms/step - loss: 0.2909 - acc: 0.8862 - val_loss: 0.2602 - val_acc: 0.9033\n",
      "Epoch 3/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2758 - acc: 0.8935 - val_loss: 0.2482 - val_acc: 0.9084\n",
      "Epoch 4/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2740 - acc: 0.8988 - val_loss: 0.2695 - val_acc: 0.8947\n",
      "Epoch 5/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2610 - acc: 0.9013 - val_loss: 0.3031 - val_acc: 0.8947\n",
      "Epoch 6/10\n",
      "709/709 [==============================] - 15s 21ms/step - loss: 0.2498 - acc: 0.9074 - val_loss: 0.2428 - val_acc: 0.9143\n",
      "Epoch 7/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2464 - acc: 0.9079 - val_loss: 0.2541 - val_acc: 0.9094\n",
      "Epoch 8/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2601 - acc: 0.9096 - val_loss: 0.2455 - val_acc: 0.9078\n",
      "Epoch 9/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2586 - acc: 0.9071 - val_loss: 0.2717 - val_acc: 0.9037\n",
      "Epoch 10/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2558 - acc: 0.9102 - val_loss: 0.2387 - val_acc: 0.9171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit!\n",
    "history = model.fit(\n",
    "    img_train,\n",
    "    validation_data=(img_test),\n",
    "    batch_size=128,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ba2f6-4096-4366-aea7-71d6a10acf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1cb8a-bd99-4036-baee-53172ea75aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ef6ae-acef-4d40-aac2-71172eb18d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a6bd4-3eb2-4361-8c52-c67e0e849ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaac16-f2f3-4e31-a0d1-859e5f434088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd5525-3679-43d5-b770-4f3afa1f6604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ae802-2c28-458e-8897-f4bbdc563f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27703e8-0b77-4687-80be-a8bf5e2ad00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e8460-f128-4a03-8253-17375c5601c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee61c35-10e6-4e49-bab5-cda301f3745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef842cec-5be2-4c29-be4c-0006091223a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea109-2cc1-46d8-a8ee-42302d7b66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbda3aac-d9ab-4c18-ab4f-a9a265aabf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_img_array = np.array(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87aaab15-6fba-454f-9a85-ca9d7459d239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f953dca-864b-4b1c-a666-15ca86bebabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'io' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m all_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/train/nowildfire/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m(image_path , as_grey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m   img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mreshape([WIDTH, HEIGHT, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m   all_images\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'io' has no attribute 'imread'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "\n",
    "all_images = []\n",
    "for image_path in os.listdir('./data/train/nowildfire/'):\n",
    "  img = io.imread(image_path , as_grey=True)\n",
    "  img = img.reshape([WIDTH, HEIGHT, 1])\n",
    "  all_images.append(img)\n",
    "X_train = np.array(all_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638633f9-eb26-49f0-aea0-450014c323e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442193f9-3f17-4668-8583-c6261c602285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d056d0b1-1182-48a2-8f23-310f6e4b8a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002616ef-b97a-463f-9415-997bd18a6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.utils import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bf29d-7841-4078-8bf9-9c16dfa934b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('./data/train/nowildfire/*.jpg'):\n",
    "    img = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
