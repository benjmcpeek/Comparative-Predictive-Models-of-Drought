{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06309ab3-0e89-49cd-95d3-a7342dd8b8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75dffb-a7d2-4d19-8715-c52d93553175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import cv2\n",
    "\n",
    "# images = [cv2.imread(file) for file in glob.glob('./data/train/nowildfire/*.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c85c1-c578-4fbf-bc7f-f0e4f789e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c927f-533e-4452-9e07-a06fbd9db928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a1c8b-7f95-4af3-9406-51f67087783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e454ab-5584-48c7-bb46-695a1cbd1285",
   "metadata": {},
   "source": [
    "# Part 3: Image Classification Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aef37c-8293-4e6b-9384-15d66482fd77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook Summary\n",
    "\n",
    "This notebook explores our second model in which satellite images are classified to identify whether an image has or has not experienced a wildfire. This second model assumes that wildfire areas exhibit many similar terrestrial characteristics to areas prone to drought, such a drier vegetation and soils as well as potentially sparser vegetation. If a convolutional nearual network model could be trained on wildfire and no wildfire satellite images, then a similar model coud be used on satellite images to predict drought. In this notebook, the reader will find:\n",
    "\n",
    "* Data Collection Methods\n",
    "* Image Preprocessing\n",
    "* Baseline CNN Model\n",
    "* Model Tuning\n",
    "* Production Model & Evaluation\n",
    "* Notebook Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41184f-e095-4cf0-9f8d-f6952c8121c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Collection Methods\n",
    "\n",
    "The following dataset was collected from a Kaggle Wildfire Prediction Dataset [source](https://www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset?select=train). The original dataset was compiled by the Kaggle contributor from Canada's government website, sourced from the government and municipalities of Quebec, which compiled images primarily from southern Quebec, dating back to 1976 [source](https://open.canada.ca/data/en/dataset/9d8f219c-4df0-4481-926f-8a2a532ca003). According to the Kaggle webpage, the wildfire images include those which contain greater than 0.01 acres burned. Upon cursory review of the images in both the \"wildfire\" and \"nowildfire\" classes, the \"nowildfire\" class contains images of both forested or green area as well as images of urban landscapes and human settlements, albeit not exclusively.\n",
    "\n",
    "Although the original Kaggle data contains three separate train, test, and validation datasets, we have downloaded only the train dataset since it contains 30,250 satellite images in total, of which 15,750 are classified as \"wildfire\" (\\~52%) and 14,500 are classified as \"nowildfire\" (\\~48%). The train dataset will be read into this notebook and then split into train and test datasets for training and evaluating our image classification CNN.\n",
    "\n",
    "In the subsequent section, we shall read in our image data and begin preprocessing any images, as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bd1f0-55c4-4691-93fe-8b838fee5a5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "In this section, we will begin reading in the dataset and then conducting any requisite cleaning of the data and preprocessing to make our models train and fit correctly. We will begin by importing the requisite libraries and using the image_dataset_from_directory model to create a train/test split and preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2e5dfb6-c538-48df-b9fc-b4f9f59a7220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requisite libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f7d74e-9ded-4b8c-b8a5-b40e241875aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 files belonging to 2 classes.\n",
      "Using 22688 files for training.\n",
      "Found 30250 files belonging to 2 classes.\n",
      "Using 7562 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# General Assembly instructor, Alanna Besaw, recommended investigating image_dataset_from_directory module for manipulating this dataset\n",
    "# original image sizes are 350 x 350 pixels\n",
    "# create a training set of the data representing 75% of the images\n",
    "img_train = image_dataset_from_directory('./data/train/',  \n",
    "                                         validation_split = 0.25,\n",
    "                                         image_size = (35, 35), # resized to exactly one tenth in size\n",
    "                                         subset = 'training', \n",
    "                                         seed = 42)\n",
    "\n",
    "# create a test set of the representing 25% of the images\n",
    "img_test = image_dataset_from_directory('./data/train/', \n",
    "                                         validation_split = 0.25, \n",
    "                                         image_size = (35, 35),  # resized to exactly one tenth in size\n",
    "                                         subset = 'validation', \n",
    "                                         seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5294e18f-0170-428c-91a4-cfa7d95e46f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 35, 35, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the datatype of the train set\n",
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1c94862-c2d5-49c0-979e-e95ba25c8453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 35, 35, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the datatype of the test set\n",
    "img_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947b0dd-f92c-4487-95f9-f206b18f9b3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8bfe567-6e54-4422-9ea2-299ea9593fa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = './data/train/nowildfire'\n",
    "\n",
    "# Get a list of all items (files and directories) in the directory\n",
    "all_items = os.listdir(directory)\n",
    "\n",
    "# Filter out only the file names from the list\n",
    "file_names = [item for item in all_items if os.path.isfile(os.path.join(directory, item))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eb092cb-de7d-4db2-acfd-1fc3fc1625ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from struct import unpack\n",
    "import os\n",
    "\n",
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "\n",
    "class JPEG:\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "            # print(marker_mapping.get(marker))\n",
    "            if marker == 0xffd8:\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9:\n",
    "                return\n",
    "            elif marker == 0xffda:\n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                data = data[2+lenchunk:]            \n",
    "            if len(data)==0:\n",
    "                break        \n",
    "\n",
    "\n",
    "bads = []\n",
    "\n",
    "\n",
    "for img in file_names:\n",
    "  image = os.path.join(directory, img)\n",
    "  image = JPEG(image) \n",
    "  try:\n",
    "    image.decode()   \n",
    "  except:\n",
    "    bads.append(img)\n",
    "\n",
    "\n",
    "for name in bads:\n",
    "  os.remove(os.path.join(directory,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed7d38ad-9e74-45cf-bc1a-012f06629ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8434bc22-1da8-48ef-b96d-2c48c69a5fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nowildfire', 'wildfire']\n"
     ]
    }
   ],
   "source": [
    "class_names = img_train.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16204f5a-d550-4fd9-939b-cd40d98b4fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = Rescaling(1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a2ebae4-a887-48cc-9916-6660f14a3d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.84583336\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = img_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807c543-7eed-4429-a6b7-c7d27eca41da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70410af7-6dd8-42b4-8b01-133ecec31b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Make a convolutional layer with 32 filters.\n",
    "model.add(Conv2D(32, 3, activation='relu', input_shape=(28, 28, 3)))\n",
    "\n",
    "# MaxPool the results (basically a requirement)\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Let's add another convolution block\n",
    "model.add(Conv2D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "# Finally, flatten the output and make a predictions through a dense layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f87851bd-28cf-4693-9615-cd2e018af8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='rmsprop',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cdc6896-1ccf-4dac-b442-bebc0507d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 1.0117 - acc: 0.8119 - val_loss: 0.3157 - val_acc: 0.8836\n",
      "Epoch 2/10\n",
      "709/709 [==============================] - 10s 15ms/step - loss: 0.2920 - acc: 0.8873 - val_loss: 0.2738 - val_acc: 0.8978\n",
      "Epoch 3/10\n",
      "709/709 [==============================] - 10s 14ms/step - loss: 0.2660 - acc: 0.8998 - val_loss: 0.3084 - val_acc: 0.8876\n",
      "Epoch 4/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2615 - acc: 0.9010 - val_loss: 0.2466 - val_acc: 0.9105\n",
      "Epoch 5/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2496 - acc: 0.9078 - val_loss: 0.3280 - val_acc: 0.8853\n",
      "Epoch 6/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2479 - acc: 0.9085 - val_loss: 0.3267 - val_acc: 0.8666\n",
      "Epoch 7/10\n",
      "709/709 [==============================] - 12s 16ms/step - loss: 0.2414 - acc: 0.9098 - val_loss: 0.6318 - val_acc: 0.7622\n",
      "Epoch 8/10\n",
      "709/709 [==============================] - 13s 19ms/step - loss: 0.2434 - acc: 0.9123 - val_loss: 0.2914 - val_acc: 0.8996\n",
      "Epoch 9/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2488 - acc: 0.9101 - val_loss: 0.2479 - val_acc: 0.9172\n",
      "Epoch 10/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2478 - acc: 0.9116 - val_loss: 0.2714 - val_acc: 0.9140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit!\n",
    "history = model.fit(\n",
    "    img_train,\n",
    "    validation_data=(img_test),\n",
    "    batch_size=512,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabf0cb-569c-459b-bbce-87a709bcc4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c00b955-6452-484f-94a1-b30900908966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac04bc72-d2a3-4751-812c-4fd4903e1145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1221c5d6-edf5-440d-9f65-91380b9434b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "709/709 [==============================] - 11s 14ms/step - loss: 0.8152 - acc: 0.8180 - val_loss: 0.4151 - val_acc: 0.8412\n",
      "Epoch 2/10\n",
      "709/709 [==============================] - 19s 26ms/step - loss: 0.2909 - acc: 0.8862 - val_loss: 0.2602 - val_acc: 0.9033\n",
      "Epoch 3/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2758 - acc: 0.8935 - val_loss: 0.2482 - val_acc: 0.9084\n",
      "Epoch 4/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2740 - acc: 0.8988 - val_loss: 0.2695 - val_acc: 0.8947\n",
      "Epoch 5/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2610 - acc: 0.9013 - val_loss: 0.3031 - val_acc: 0.8947\n",
      "Epoch 6/10\n",
      "709/709 [==============================] - 15s 21ms/step - loss: 0.2498 - acc: 0.9074 - val_loss: 0.2428 - val_acc: 0.9143\n",
      "Epoch 7/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2464 - acc: 0.9079 - val_loss: 0.2541 - val_acc: 0.9094\n",
      "Epoch 8/10\n",
      "709/709 [==============================] - 11s 15ms/step - loss: 0.2601 - acc: 0.9096 - val_loss: 0.2455 - val_acc: 0.9078\n",
      "Epoch 9/10\n",
      "709/709 [==============================] - 11s 16ms/step - loss: 0.2586 - acc: 0.9071 - val_loss: 0.2717 - val_acc: 0.9037\n",
      "Epoch 10/10\n",
      "709/709 [==============================] - 12s 17ms/step - loss: 0.2558 - acc: 0.9102 - val_loss: 0.2387 - val_acc: 0.9171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Fit!\n",
    "# history = model.fit(\n",
    "#     img_train,\n",
    "#     validation_data=(img_test),\n",
    "#     batch_size=128,\n",
    "#     epochs=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ba2f6-4096-4366-aea7-71d6a10acf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1cb8a-bd99-4036-baee-53172ea75aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ef6ae-acef-4d40-aac2-71172eb18d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a6bd4-3eb2-4361-8c52-c67e0e849ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaac16-f2f3-4e31-a0d1-859e5f434088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd5525-3679-43d5-b770-4f3afa1f6604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ae802-2c28-458e-8897-f4bbdc563f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27703e8-0b77-4687-80be-a8bf5e2ad00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e8460-f128-4a03-8253-17375c5601c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee61c35-10e6-4e49-bab5-cda301f3745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef842cec-5be2-4c29-be4c-0006091223a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea109-2cc1-46d8-a8ee-42302d7b66cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda3aac-d9ab-4c18-ab4f-a9a265aabf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaab15-6fba-454f-9a85-ca9d7459d239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f953dca-864b-4b1c-a666-15ca86bebabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638633f9-eb26-49f0-aea0-450014c323e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442193f9-3f17-4668-8583-c6261c602285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056d0b1-1182-48a2-8f23-310f6e4b8a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002616ef-b97a-463f-9415-997bd18a6108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bf29d-7841-4078-8bf9-9c16dfa934b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a3d74-8aa9-413e-9321-02d354ac479e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
